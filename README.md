ğŸ’¬ BeyondChats â€” AI-Powered Content Automation Platform

BeyondChats is a full-stack web application that automates blog content enhancement using web scraping, Google search intelligence, and AI rewriting.
It transforms existing articles into richer, structured, and SEO-friendly versions â€” while preserving original content and citing trusted sources.

Built with a real-world backend automation mindset, this project demonstrates scraping pipelines, AI integration, and clean frontend delivery.

ğŸš€ Hosted Links

ğŸ”— Frontend: https://beyondchats-theta.vercel.app/
ğŸ”— Backend API: https://beyondchats-0vmu.onrender.com

ğŸ”— Articles API: https://beyondchats-0vmu.onrender.com/api/articles

âœ¨ Features
ğŸ“° Article Scraping

Scrapes the oldest blog articles from BeyondChats

Extracts titles, content, and source URLs

Skips malformed or low-quality pages automatically

Prevents duplicates using unique slugs (idempotent scraping)

ğŸ§  AI-Powered Article Enhancement

Searches Google for top external reference articles

Scrapes content from trusted third-party blogs

Uses an LLM (Groq) to rewrite articles:

Better structure

Clear headings

SEO-friendly flow

No plagiarism

Publishes updated versions alongside originals

ğŸ“š Article Management

Original and AI-updated articles stored separately

Clear isUpdated flag for distinction

Reference URLs attached to updated articles

Full CRUD support via REST APIs

ğŸ¨ Frontend Experience

Clean, modern React UI

Clearly marked AI-Updated articles

Read-more / read-less interaction

Clickable references

Responsive and minimal design

ğŸ§© Tech Stack
Frontend

React (Vite)

Tailwind CSS

Axios

Backend

Node.js

Express.js

MongoDB Atlas

Mongoose

Axios + Cheerio (Scraping)

SerpAPI (Google Search)

Groq LLaMA 3.1 (AI Rewriting)

Hosting

Frontend â†’ Vercel

Backend â†’ Render

Database â†’ MongoDB Atlas

ğŸ“¦ Project Structure
root
â”‚
â”œâ”€â”€ frontend/        â†’ React + Tailwind client
â””â”€â”€ backend/         â†’ Node.js + Express REST API

ğŸ”— API Routes (Backend)
ğŸ“° Articles

GET /api/articles â€” Fetch all articles

POST /api/articles â€” Create article

PUT /api/articles/:id â€” Update article

DELETE /api/articles/:id â€” Delete article

âš™ï¸ Automation

POST /api/scrape â€” Scrape BeyondChats articles

POST /api/publish â€” Generate & publish AI-updated article

ğŸ§  System Flow
BeyondChats Blog Page
        â†“
Web Scraper (Axios + Cheerio)
        â†“
MongoDB (Original Articles)
        â†“
Google Search (SerpAPI)
        â†“
External Article Scraping
        â†“
AI Rewriting (Groq LLM)
        â†“
MongoDB (Updated Articles)
        â†“
React Frontend (Vercel)

ğŸ” Why You May See Fewer Than 5 Articles

The scraper targets the 5 oldest articles, but fewer may appear due to intentional safeguards:

Articles without valid titles or sufficient content are skipped

Duplicate articles are prevented using unique slugs

Test or placeholder entries are excluded

This ensures:

No empty or broken content

Clean, reliable data

Production-grade scraping behavior

Data quality is prioritized over forcing a fixed count.

ğŸ› ï¸ Local Setup
Backend
cd backend
npm install
npm run dev


Create .env:

PORT=3000
MONGO_URI=your_mongodb_atlas_uri
GROQ_API_KEY=your_groq_key
SERPAPI_KEY=your_serpapi_key

Frontend
cd frontend
npm install
npm run dev


Create frontend/.env:

VITE_API_URL=http://localhost:3000/api

ğŸŒ Deployment Notes
Backend (Render)

Root directory: backend

Build: npm install

Start: npm start

Environment variables set via Render dashboard

MongoDB Atlas network access enabled

Frontend (Vercel)
VITE_API_URL=https://beyondchats-0vmu.onrender.com/api

âš ï¸ Challenges & Decisions

Scraping reliability:
Implemented validation to skip malformed or blocked pages.

Duplicate handling:
Idempotent logic ensures safe re-runs without data pollution.

LLM choice:
Switched from OpenAI to Groq for faster inference and free-tier reliability.

Production readiness:
Clear separation of local vs deployed environment variables.
